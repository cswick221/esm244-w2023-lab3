---
title: "Lab 3"
author: "Chloe Swick"
date: "2023-01-26"
output: html_document
---

```{r setup, include=TRUE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(tidymodels)
library(palmerpenguins)
library(GGally)
library(jtools)
library(AICcmodavg)
```

# Pseudocode 

* examine our data, tables, summary stats 
* identify a question 
* wrangle data if necessary 
* identify some candidate models 
* select among candidates models using AIC/BIC 
* select among candidate models using k-fold cross validation 
* select among candidate models using area under receiver operating characteristic curve 


```{r}
GGally::ggpairs(penguins %>% select(species, bill_length_mm:sex),
                aes(color = species))
## quick way to look at information about data
```

```{r}
class(penguins$species)
levels(penguins$species)
```

```{r}
adelie_chinstrap <- penguins %>% 
  filter(species %in% c('Adelie', 'Chinstrap')) %>% 
  mutate(species = fct_drop(species)) %>% 
  select(-year) %>% 
  drop_na()
levels(adelie_chinstrap$species)
```


== means a match, when using with a vector, it matches the pattern as well 
%in% means a match to either or any of the items in vector 


```{r}
ggplot(data = adelie_chinstrap, aes(x = body_mass_g, y = flipper_length_mm))+
  geom_point(aes(color = sex, shape = island)) +
  facet_wrap(~species)

ggplot(data = adelie_chinstrap, aes(x = body_mass_g, y = bill_length_mm)) +
  geom_point(aes(color = sex, shape = island)) +
  facet_wrap(~species) # facet_wrap makes two different graphs per how many categoeries of the vairable you choose, in this example, we have a plot of adelie, and a plot for chinstrap 
```

# for fun 
```{r}
ggplot(data = adelie_chinstrap, aes(x = body_mass_g, y = bill_length_mm)) +
  geom_point(aes(color = sex, shape = species)) +
  facet_wrap(~island)
```



# binary logistic regression 
```{r}
f1 <- species ~ body_mass_g + flipper_length_mm + sex 

ad_chin_blr1 <- glm(formula = f1, data = adelie_chinstrap, 
                    family = 'binomial')
ad_chin_blr1
summary(ad_chin_blr1)

blr1_tidy <- tidy(ad_chin_blr1) # creates a table of summary statistics 
```


```{r}
ggplot(data = adelie_chinstrap, aes(x = species, y = flipper_length_mm))+
  geom_jitter(aes(color = sex))
```



```{r}
blr1_fitted <- ad_chin_blr1 %>% 
  broom::augment(type.predict = 'response') #how good is our model based off binary logistic regression 
```

```{r}
ggplot(data = blr1_fitted, aes(x = flipper_length_mm, y = .fitted))+
  geom_point(aes(color = sex, shape = species))+
  geom_smooth(aes(color = sex), se = FALSE)+
  labs(x = 'flipper length (mm)', y = 'probability of outcome (chinstrap)')
```


## predictions for new values with predict()

```{r}
ex_1 <- predict(ad_chin_blr1,
                data.frame(sex = "female",
                  body_mass_g = 3410,
                  flipper_length_mm = 192),
                # tell it type = 'response' to get prob, not log odds
                type = "response")
 
```


```{r}
new_df <- data.frame(
  sex = c('male', 'male', 'female'),
  body_mass_g = c(3298, 4100, 3600),
  flipper_length_mm = c(212, 175, 180)
)

ex2<- predict(ad_chin_blr1, new_df, type = 'response')
ex2
```


```{r}
f2 <- species ~ bill_length_mm + body_mass_g


ad_chin_blr2 <- glm(formula = f2,
                    data = adelie_chinstrap,
                    family = "binomial")
```


```{r}
ad_chin_blr2
 
summary(ad_chin_blr2)
 
# Get a tidy version w/ broom:
blr2_tidy <- broom::tidy(ad_chin_blr2)
```


```{r}
ggplot(adelie_chinstrap, aes(x = bill_length_mm, y = body_mass_g)) +
  geom_point(aes(color = species))
```

Let's visualize the results for this model like we did before:
``` {r}
effect_plot(ad_chin_blr2,
        	pred = bill_length_mm,
        	interval = TRUE,
        	y.label = "Probability of 'Chinstrap'")


effect_plot(ad_chin_blr2,
        	pred = body_mass_g,
        	interval = TRUE,
        	y.label = "Probability of 'Chinstrap'")


```

## Model selection

Let's compare the models using AICc and BIC
```{r}
AICcmodavg::aictab(list(ad_chin_blr1, ad_chin_blr2))
AICcmodavg::bictab(list(ad_chin_blr1, ad_chin_blr2))
```
* the log likely hood is LL 
difference of 10 is delta AIC is good, so 200 is ovewhelmingly worse 

Delta BIC is slightly different because they penalize things differently but still shows that mdl2 is much better 

And let's compare with a 10-fold cross-validation, using prediction accuracy as our metric.
*cross validation is used in more complex analysis like machine learning 

``` {r}
set.seed(123) # number you set seed to is arbitrary 


n_folds <- 10
fold_vec <- rep(1:n_folds, length.out = nrow(adelie_chinstrap)) # length.out is the number of rows of our adelie chinstrap data frame 
ad_chin_kfold <- adelie_chinstrap %>%
  mutate(fold = sample(fold_vec, size = n(), replace = FALSE)) # size n() is the number of observations in data frame, 214 

```

# for-loop version (SKIP FOR LAB - include as reference)

```{r}
results_df <- data.frame()
pred_acc <- function(x, y) {
  accurate <- ifelse(x == y, 1, 0)
  return(mean(accurate, na.rm = TRUE))
}

for(i in 1:n_folds) {
  kfold_test <- ad_chin_kfold %>%
    filter(fold == i)
  kfold_train <- ad_chin_kfold %>%
    filter(fold != i)
  
  kfold_blr1 <- glm(f1, data = kfold_train, family = 'binomial')
  kfold_blr2 <- glm(f2, data = kfold_train, family = 'binomial')
  kfold_pred <- kfold_test %>%
    mutate(blr1 = predict(kfold_blr1, kfold_test, type = 'response'),
           blr2 = predict(kfold_blr2, ., type = 'response')) %>%
    mutate(pred1 = ifelse(blr1 > 0.50, 'Chinstrap', 'Adelie'),
           pred2 = ifelse(blr2 > 0.50, 'Chinstrap', 'Adelie'))
  kfold_accuracy <- kfold_pred %>%
    summarize(blr1_acc = pred_acc(species, pred1),
              blr2_acc = pred_acc(species, pred2))
  
  results_df <- bind_rows(results_df, kfold_accuracy)
}


results_df %>%
  summarize(blr1_acc = mean(blr1_acc),
            blr2_acc = mean(blr2_acc))
```

# purrr::map version: returns a list
as alternative to for_loop shown above 

```{r}
x_vec <- 1:10

thing <- purrr::map(.x = x_vec, # a sequence (vector, list, etc.)
                    .f = sqrt)  # name of a function (without parens)

# here we are doing an example of running a function "sqrt" on a sequence, the x_vec we created above

my_funct <- function(x, y, z) {
  return((x - y) ^ z)
}

thing2 <- purrr::map(.x = x_vec,      # a sequence (for first arg of function)
                     .f = my_funct,   # name of a function to apply
                     y = 2, z = 3)    # additional parameters (for other args)
```

``` {r}
# function to calculate accuracy, given a "truth" vector and "prediction" vector
pred_acc <- function(x, y) {
  accurate <- ifelse(x == y, 1, 0) # 1 if it matches y, 0 if it doesnt 
  
  return(mean(accurate, na.rm = TRUE))
}

# is element one equal to my predicted element 1? if so return 1, and so on 

# function to calculate accuracy of BLR of one fold (training and testing)
calc_fold <- function(i, fold_df, f) {
  kfold_test <- fold_df %>% #fold_df hasnt been defined yet 
    filter(fold == i) #i so fold matches whatever item in sequence we are on
  kfold_train <- fold_df %>%
    filter(fold != i) #fold not equal to 'i'
  
  kfold_blr <- glm(f, data = kfold_train, family = 'binomial')
  kfold_pred <- kfold_test %>%
    mutate(blr = predict(kfold_blr, kfold_test, type = 'response')) %>%
    mutate(pred = ifelse(blr > 0.50, 'Chinstrap', 'Adelie')) #ifelse will take vector with many values and compare each value to >.5, if true put chinstrap, if not, put adelie, into new column called "pred"
  
  kfold_accuracy <- kfold_pred %>%
    summarize(blr_acc = pred_acc(species, pred)) # using my pred_acc function, created above 
  
  return(kfold_accuracy)
}

n_folds <- 10

results1_purrr_df <- purrr::map(.x = 1:n_folds, # sequence of fold numbers
                                .f = calc_fold, # function
                                fold_df = ad_chin_kfold, # additional argument to calc_fold()
                                f = f1) %>%              # additional argument to calc_fold() f1 is the first model 
  bind_rows() %>%
  mutate(mdl = 'f1')

results2_purrr_df <- purrr::map(.x = 1:n_folds, .f = calc_fold, 
                               fold_df = ad_chin_kfold,
                               f = f2) %>%  # f2 is the second model 
  bind_rows() %>%
  mutate(mdl = 'f2')

results_purrr_df <- bind_rows(results1_purrr_df, results2_purrr_df) %>%
  group_by(mdl) %>%
  summarize(mean_acc = mean(blr_acc)) #mean_acc is mean accuracy, so mean of binomial linear regression 

results_purrr_df # results show the percent of time they predicted accurately, so model 2 is better. 
```

Which model seems best?  Does this agree with AIC and BIC selection?
model 2 is accurate 97% of the time, AIC and BIC also showed f2 to be better 


# Tidymodels flow

See https://www.tidymodels.org/ for tons of details and tutorials!  Tidymodels (and parsnip) packages clean up and standardize the output from hundreds of different modeling functions from dozens of different modeling packages.  For example, binomial logistic regression algorithms show up in quite a few different modeling packages, but the arguments and outputs differ from package to package - annoying!

Not going to get into: "recipes" for pre-processing, "workflows" 

## Tidymodels basic

```{r}
### Set the model type
?logistic_reg ### note glm is the default engine

blr_model <- logistic_reg() %>% ### also linear_reg, rand_forest, etc
  set_engine('glm')

### basic regression
blr_tidyfit_f1 <- blr_model %>%
  fit(f1, data = adelie_chinstrap)
blr_tidyfit_f2 <- blr_model %>%
  fit(f2, data = adelie_chinstrap)

### query the fitted models
blr_tidyfit_f1
blr_tidyfit_f2

### examine different outputs to see how well the models fit
blr_tidyfit_f1 %>%
  tidy()

blr_tidyfit_f2 %>%
  glance()

```

## Tidymodels crossfold validation

```{r}
### set seed for reproducibility! here to set the folds
set.seed(345)

tidy_folds <- vfold_cv(adelie_chinstrap, v = 10)
tidy_folds

### use a workflow that bundles the logistic model and a formula
# blr_model <- logistic_reg() %>%
#   set_engine('glm')

blr_tidy_wf1 <- workflow() %>% #workflows are built into tidy package-- confusing 
  add_model(blr_model) %>%
  add_formula(f1)

blr_tidy_cv_f1 <- blr_tidy_wf1 %>%
  fit_resamples(tidy_folds)

### use functions from the tune package to extract metrics
collect_metrics(blr_tidy_cv_f1)
#   .metric  .estimator  mean     n std_err .config             
#   <chr>    <chr>      <dbl> <int>   <dbl> <chr>               
# 1 accuracy binary     0.828    10 0.00739 Preprocessor1_Model1
# 2 roc_auc  binary     0.902    10 0.00808 Preprocessor1_Model1

### We'll talk about roc_auc next week!


### Repeat for model 2 - let students do this on their own!
blr_tidy_wf2 <- workflow() %>%
  add_model(blr_model) %>%
  add_formula(f2)

blr_tidy_cv_f2 <- blr_tidy_wf2 %>%
  fit_resamples(tidy_folds)

### use functions from the tune package to extract metrics
collect_metrics(blr_tidy_cv_f2)

```

## Area under the curve!

Receiver Operating Characteristic Curve (ROC Curve) compares the diagnostic ability of a binary classifier (like logistic regression) based on the discrimination threshold.  Up to now (and for homework) we've been using a 50% threshold by default.  The ROC can tell us tradeoffs between true positive rate and false positive rate as we change the threshold, and also can give a great indication of model quality.

It seems like model 2 is far better than model 1 in this instance.

```{r}
### This is copied from above, for reference
# blr_model <- logistic_reg() %>% ### also linear_reg, rand_forest, etc
#   set_engine('glm')
# 
# ### basic regression
# blr_tidyfit_f1 <- blr_model %>%
#   fit(f1, data = adelie_chinstrap)
# blr_tidyfit_f2 <- blr_model %>%
#   fit(f2, data = adelie_chinstrap)

blr_f1_pred <- adelie_chinstrap %>%
  mutate(predict(blr_tidyfit_f1, .),
         predict(blr_tidyfit_f1, ., type = 'prob'))

blr_f1_pred %>%
  roc_curve(truth = species, .pred_Adelie) %>%
  autoplot()

blr_f1_pred %>%
  roc_auc(truth = species, .pred_Adelie)

### Students repeat for blr_tidyfit_f2 and compare!
blr_f2_pred <- adelie_chinstrap %>%
  mutate(predict(blr_tidyfit_f2, .),
         predict(blr_tidyfit_f2, ., type = 'prob'))

blr_f2_pred %>%
  roc_curve(truth = species, .pred_Adelie) %>%
  autoplot()

blr_f2_pred %>%
  roc_auc(truth = species, .pred_Adelie)

```

x is false positives 
and y is true positives 

# End Part 1
















